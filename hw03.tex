\documentclass[a4paper]{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[left=1.8cm,right=1.8cm,top=2.2cm,bottom=2.0cm]{geometry}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{xpatch}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subfigure}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{framed}
\usepackage{multicol}
\usepackage{fontspec}
\usepackage{float}
\usepackage{algpseudocode}
\usepackage{extarrows}
\usepackage{algorithm}
\usepackage{tikz}
\makeatletter

\printanswers


\AtBeginDocument{\xpatchcmd{\@thm}{\thm@headpunct{.}}{\thm@headpunct{}}{}{}}
\makeatother

\pagestyle{fancy}
\renewcommand{\baselinestretch}{1.15}
\newcommand{\code}[1]{\texttt{#1}}
\usepackage{paralist}
\let\itemize\compactitem
\let\enditemize\endcompactitem
\let\enumerate\compactenum
\let\endenumerate\endcompactenum
\let\description\compactdesc
\let\enddescription\endcompactdesc

% shorten footnote rule
\xpatchcmd\footnoterule
  {.4\columnwidth}
  {1in}
  {}{\fail}

\title{CSE 201: Homework 3}
% \author{\textbf{Yiwei Yang} \\ \texttt{ yyang363@ucsc.edu}}


\begin{document}
\maketitle
\section{The following questions are about the Worst Case and the Best Case performance of QuickSort. }
\begin{algorithm}
  \caption{QUICKSORT$(A,p,r)$}\label{alg:cap1}
  \begin{algorithmic}[1]
    % \Require $n \geq 0$
    % \Ensure $y = x^n$
    \If{$p<r$}
    \State $q =\text{PARTITION}(A,p,r) $
    \State $\text{QUICKSORT}(A,p, q-1)$
    \State $\text{QUICKSORT}(A,q+1,r)$
    \EndIf
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{PARTISION$(A,p,r)$}\label{alg:cap2}
  \begin{algorithmic}[1]
    % \Require $n \geq 0$
    % \Ensure $y = x^n$
    \State $x=A[r]$
    \State $i=p-1$

    \For{$j=p$ \textbf{to} $r-1$}
    \If{ $A[j]\leq x $}
    \State $i=i+1$
    \State exchange $A[i]$ with $A[j]$
    \EndIf
    \State exchange $A[i+1]$ with $A[r]$
    \EndFor
    \\
    \Return i+1
  \end{algorithmic}
\end{algorithm}
\subsection{The Worst Case Running Time of QuickSort }

on an array with $\mathrm{n}$ items is $\theta\left(\mathrm{n}^2\right)$, because an Adversary might pick an input of size $\mathrm{n}$ such that the Running Time for QuickSort on that input will be both $O\left(n^2\right)$ and $\Omega\left(n^2\right)$.
\begin{proof}
  The adversary case of the worst case is the least balanced partition that produce two subproblem with $n-1$ elements. So that one partitioning costs the $\Theta(n) $ time while the other costs $T(0)$ time. Then we gain that $T(n)=max\{T(n-1-q)+T(q) : 0\leq q\leq n-1\}+ \Theta(n)$. Using the substitution methods that we guess that $T(n)\leq O(n^2)$
  We have
  $$\begin{aligned} T(n) & \leq \max \left\{c_1 q^2+c_1(n-1-q)^2: 0 \leq q \leq n-1\right\}+\Theta(n) \\ &=c_1 \cdot \max \left\{q^2+(n-1-q)^2: 0 \leq q \leq n-1\right\}+\Theta(n) . \
                \\ &= c_1 \cdot max \{q^2+(n-1)^2-2 q(n-1)+q^2\} + \Theta(n)
                \\ &=c_1 \cdot max\{(n-1)^2+2q(q-(n-1)) \}
                \\ &\leq c_1 \cdot(n-1)^2\end{aligned}$$

  $$\forall n_0 \geq 1, c_1\geq 1$$

  Thus we have $T(n)\in O(n^2)$.

  The adversary case of the worst case is that produces one subproblem with $n – 1$ elements and one with 0 elements.
  $$\begin{aligned} T(n) & \leq c_2(n-1)^2+\Theta(n) \\ & \leq c_2(n-1)^2+c_0 n \\ & \leq c_2\left(n^2-2 n+1\right)+c_0 n \\ & \leq c_2 n^2-\left(2 c_2-c_0\right) n+c_2 \\ & \leq c_2 n^2 \end{aligned}$$

  $$\forall n_0 \geq 1, c_0>c_2$$

  Thus we have $T(n)\in \Omega(n^2)$.

\end{proof}

\subsection{What is the Best Case Running Time of QuickSort on an array with n items?  Prove that your answer is the Best Case.}

\begin{proof}
  \textbf{PARTITION} generates two in the most evenly split conceivable scenario that the partitioning is equally balanced at every level of the rescursion. Since there's total size of $n - 1$, there are 2 subproblems, each of size no greater than $\frac{n}2$. One of size $\frac{\lfloor n - 1\rfloor}2 - 1 $ and one of size $\frac{\lfloor n - 1\rfloor}2$. Here, use quicksort runs a lot quicker. The running time can then be limited to an upper limit as evidenced by the occurrence.

  $$T(n)=2 T(n / 2)+\Theta(n)$$

  Using the second case of master Theorem, we have $T(n)=\Theta(n \lg n)$
\end{proof}

\section{Using substitution method}

\subsection{Using the Substitution Method, try to prove that the solution of the Recurrence is $O(n^4)$}
$T(1)=1$\\
For $n \geq 2$\\
$\quad T(n)=T(n-1)+n^3$
\begin{proof}
  We have
  $$\begin{aligned} T(n) & \leq c(n-1)^4+n^3
                \\ & \leq c(n^4-4n^3+6n^2-4n +1)+ n^3
                \\ & \leq cn^4-(4c-1)n^3+6cn^2-4cn +c
    \end{aligned}$$

  Take $c=\frac 14,n_0\geq 2$ to let $cn^4-(4c-1)n^3+6cn^2-4cn+c\leq cn^4$, we have $\frac 3 2 n^2-n+\frac 14\leq0$, check for $n\geq 2$ the equation always holds. Thus $T(n)=O(n^4)$
\end{proof}
\subsection{Using the Substitution Method, try to prove that the solution of the Recurrence is $O(n^2)$}
$T(1)=1$ \\For $n \geq 2$\\
$T(n)=2 T(\lfloor n / 2\rfloor)+2 n \quad$ (You may solve this using $n / 2$ instead of $\lfloor n / 2\rfloor$ )
\begin{proof}
  We have
  $$\begin{aligned} T(n) & \leq 2c\lfloor n^2 / 2\rfloor+2n
                \\ & \leq cn^2+2n
    \end{aligned}$$
\end{proof}

To let $-n-cn^2\leq 0$ always holds for $n\geq 2$. We have $f(n)= -c(n-\frac 1{2c})^2+\frac1{4c}\leq0$, we need to have
$\begin{cases}
    f(2) \leq 0 \\
    c>0
  \end{cases}$
So, $c\geq \frac 12$. We just take $c=1,n_0\geq 2$ will satisfy the problem.
\section{Given an array $A[1..n]$, a pair of array indices $(i, j)$ is called an Inversion if $i < j$ but $A[i] > A[j]$. That is, $A[i]$ and $A[j]$ are out-of-order in $A$.}

For example, if $n=4$ and $A[1]=13, A[2]=5, A[3]=17$ and $A[4]=8$, then there are 3 Inversions, namely $(1,2),(1,4)$ and $(3,4)$

Write an algorithm that counts the number of inversions in $A[1 . . n]$. You also need to analyze the Asymptotic Running Time of your algorithm, and provide a proof that your Running Time is correct.

(In CSE 201, "Running Time" always means "Worst Case Running Time", unless we specify something else.)

Please keep $f(n)$ simple. For example, $\theta(n)$ is a better answer than $\theta(17 n+1000)$. Some credit will be deduced for an answer such as $\theta(17 n+1000)$, even if it is correct

\begin{algorithm}
  \caption{COUNT-INVERSION$(A,p,r)$}\label{alg:cap3}
  \begin{algorithmic}[1]
    % \Require $n \geq 0$
    % \Ensure $y = x^n$
    \If{$p\geq r$}
    \State \textbf{return} 0
    \EndIf

    \State $q =\lfloor\frac{p+2}2\rfloor $
    \State $l =\text{COUNT-INVERSION}(A,p, q-1)$
    \State $r = \text{COUNT-INVERSION}(A,q+1,r)$
    \State $inv =l+r+ \text{MERGE}(A,p,q,r)$

  \end{algorithmic}
\end{algorithm}
\begin{algorithm}
  \caption{MERGE$(A,p,r)$}\label{alg:cap4}
  \begin{algorithmic}[1]
    \State $n_1=q-p+2$
    \State $n_2=r-q$
    \State \textbf{initialize} $L[1..n_1]$ and $R[1..n_2]$

    \For {$i=n$ \textbf{to} $n_1$}
    \Statement $L[i]=A[p+i-1]$
    \EndFor

    \For {$j=1$ \textbf{to} $n_2$}
    \Statement $R[j]=A[q+j]$
    \EndFor

    \State $L\left[n_1+1\right]=\infty $
    \State $R\left[n_2+1\right]=\infty $
    \State $i=1 $
    \State $j=1 $
    \State $ inv =0 $
    \For {$k=p$ \textbf { to } $r $}
    \If  {$L[i] \leq R[j] $}
    \State $ A[k]=L[i] $
    \State $i=i+1 $
    \Else
    \State $inv=inv+\left(n_1-i+1\right) $
    \State $A[k]=R[j] $
    \State $j=j+1$
    \EndIf
    \EndFor
  \end{algorithmic}
\end{algorithm}
\begin{proof}
 The worst case running time is $\Theta(n^2)$.

 \textbf{Divide}: The divide step just computes the middle of the subarray, which takes constant time. Thus, $D(n)=\Theta(1)$.

\textbf{Conquer}: Recursively solving two subproblems, each of size $n / 2$, contributes $2 T(n / 2)$ to the running time. The floors have been ignored since it doesn't affect the eventual result

\textbf{Combine}: Since the MERGE procedure on an $n$-element subarray takes $\Theta(n)$ time, we have $C(n)=\Theta(n)$.

The functions $\Theta(n)$ and $\Theta(1)$ are added when the functions $D(n)$ and $C(n)$ are added for the merge sort analysis. The linear function of $n$ is this sum. Since the division and combining times of the merge sort are $\Theta(n)$ when $n$ is large, it follows that it is roughly proportional to $n$ when $n$ is large. The recurrence for the merge sort's worst-case running time, $T(n)$, is obtained by adding $\Theta(n)$ to the $2 T(n / 2)$ term from the conquer step:

$$
T(n)=2 T(n / 2)+\Theta(n) 
$$
Using master theorem, we get the result is $\Theta(n^2)$
\end{proof}
\section{Proof master thereom}
\subsection{Prove Case 1: If $a>b^d$ then $T(n)=\Omega\left(n^{ \log _b(a)}\right)$}
\begin{proof}
  The solution of the recurrence is $T(n)=\sum_{i=0}^{\log _b n} a^i f\left(n / b^i\right)+O\left(n^{\log _b a}\right)$

  From the above equation, we have $T(n)=\sum_{i=0}^{\log _b n} a^i T\left(n / b^i\right)+O\left(n^{\log _b a}\right) \leq \sum_{i=0}^{\log _b n} a^i\left(n / b^i\right)^{\log _b a-\varepsilon}+O\left(n^{\log _b a}\right)$. Then we can deduce
$$
  \begin{aligned}
    \sum_{i=0}^{\log _b n} a^i\left(n / b^i\right)^{\log _b a-\varepsilon} &=n^{\log _b a-\varepsilon} \sum_{i=0}^{\log _b n} a^i b^{-i \log _b a} b^{i \varepsilon}=n^{\log _b a-\varepsilon} \sum_{i=0}^{\log _b n} a^i a^{-i} b^{i \varepsilon} \\
    &=n^{\log _b a-\varepsilon} \sum_{i=0}^{\log _b n} b^{\varepsilon i}
    \\ &=n^{\log _b a-\varepsilon} \frac{b^{\varepsilon\left(\log _b n+1\right)}-1}{b^{\varepsilon}-1} \\
    &=n^{\log _b a-\varepsilon} \frac{n^{\varepsilon} b^{\varepsilon}-1}{b^{\varepsilon}-1}
    \\& \geq n^{\log _b a-\varepsilon} \frac{n^{\varepsilon}( b^{\varepsilon}-1)}{b^{\varepsilon}-1}\\ 
    &=n^{\log _b a} \\
    &=\Omega\left(n^{\log _b a}\right) .
    \end{aligned}
    $$
\end{proof}
\subsection{Prove Case 3: If $a<b^d$ then $T(n)=\Omega\left(n^d\right)$}
\begin{proof}
  The lower bound is immediate, because $n^d$ is a term of the sum. $T(n)=a \cdot T(n / b)+c \cdot n^d\geq c \cdot n^d=\Omega(n^d)$
\end{proof}
\section{Strange procedure}
Here's a strange procedure, Strange(A,n), whose input is an array $A[1..n]$.
The array A is just an array of values, not a heap. But Strange does utilize both a MinHeap and a MaxHeap, which use the “Array as Binary Tree” representation that we discussed in class. That requires an array to
hold the values in the heap, (myMinHeap and myMaxHeap respectively) and a
heapSize (minSize and maxSize respectively)

\subsection{What is the time complexity of $Strange(A, n)$? }
That is, provide a function $f(n)$
such that the worst-case runtime of this algorithm is $\Theta( f(n) )$. You may assume that FindMin (and FindMax) takes constant time, and that ExtractMin (and
ExtractMax) take time $\Theta(lg n)$.
\begin{algorithm}
  \caption{STRANGE$(A,n)$}\label{alg:cap5}
  \begin{algorithmic}[1]
    \State \textbf{initialize} $maxSize=0$
    \State \textbf{initialize} $minSize=0$
    \State \textbf{initialize} $myMaxHeap=0$
    \State \textbf{initialize} $myMinHeap=0$

    \For {$j=1$ \textbf { to } $n $}
    \State MinHeapInsert($myMinHeap,minSize,A[j]$)
    \State $x=$ExtractMin($myMinHeap,minSize$)
    \State MaxHeapInsert($myMaxHeap,maxSize,x$)
    \If  {$minSize<maxSize$}
    \State $y=$ExtractMax($myMaxHeap,maxSize$)
    \State MinHeapInsert($myMinHeap,minSize,y$)
    \EndIf
    \EndFor
  \end{algorithmic}
\end{algorithm}
\begin{proof}
  First denote that for index $j$ $minSize+maxSize = j$ as $inv1$

  \textbf{Initialization}: We start by showing that the loop invariant holds before the first loop iteration, when the minHeap and maxHeap are both 0.

  \textbf{Maintenance}: For the inner loop, we first goes into the MinHeapInsert, and we extract the minimum for the MinHeap, so that we know that the MinHeap is always maintaining the bigger value once extracted to line 7. After inserting into the MaxHeap, we deduce the size of MaxHeap and MinHeap, we know that for odd number for $j$, we have the minSize smaller than maxSize, so we get into the branch to insert the biggist value of the MaxHeap to MinHeap. The running time for the loop is for odd number of $j$, takes $2\cdot \Theta(log (minSize))+\Theta(log(maxSize))$, for even number of $j$, we have $3\cdot \Theta(log (minSize))+2\cdot\Theta(log(maxSize))$ for each loop, we have $inv1$, every loop's time complexity is $\Theta(log(j))$

  \textbf{Termination}: Finally, we examine loop termination. The loop variable $j$ starts at 1 and increases by 1 in each iteration. Once $j$'s value exceeds
  n in line 5, the loop terminates. That is, the loop terminates once $j$
  equals n. Thus the time complexity is $\Sigma _1^{j=n} \Theta(log(j))= \Theta(log(n!))\xlongequal{using\ Stiring\ Formula} \Theta(nlog(n))$

  The time complexity is $\Theta(nlog(n))$
\end{proof}

\subsection{What's the value which Strange returns?}
\begin{proof}
The value is the middle one of the sorted array of $A[j]$, if the j is even, the one is the smaller one of the two middle ones, if j is odd, the one is the middle one.
\end{proof}
\end{document}
